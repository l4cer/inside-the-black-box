import numpy as np

from typing import Tuple


class Layer:
    """
    Base class for all layers in a neural network.

    Attributes
    ----------
    inputs : np.ndarray
        Stores the inputs received by the layer during forward propagation.
    outputs : np.ndarray
        Stores the outputs generated by the layer during forward propagation.
    """

    def __init__(self) -> None:
        """
        Initializes a new instance of the Layer class.
        """

        self.inputs = None
        self.outputs = None

    def init_randomly(self) -> None:
        """
        Randomly initializes the parameters of the layer.

        Raises
        ------
        NotImplementedError
            If the method is not implemented by subclasses.
        """

        raise NotImplementedError

    def dimension_compatibility(self, shape_inputs: Tuple[int]) -> Tuple[int]:
        """
        Checks the compatibility of the input tensor with the layer's requirements.

        Parameters
        ----------
        shape_inputs : Tuple[int]
            Dimensions of the input tensor.

        Raises
        ------
        NotImplementedError
            If the method is not implemented by subclasses.

        Returns
        -------
        shape_outputs : Tuple[int]
            Dimensions of the output tensor or None if not compatible.
        """

        raise NotImplementedError

    def forward_propagation(self, inputs: np.ndarray) -> np.ndarray:
        """
        Performs forward propagation of the inputs through the layer.

        Parameters
        ----------
        inputs : np.ndarray
            Input tensor for the layer.

        Raises
        ------
        NotImplementedError
            If the method is not implemented by subclasses.

        Returns
        -------
        outputs : np.ndarray
            Output tensor from the layer.
        """

        raise NotImplementedError

    def backward_propagation(self, gradient_outputs: np.ndarray,
                                   learning_rate: float) -> np.ndarray:
        """
        Performs backward propagation to adjust the parameters of the layer.

        Parameters
        ----------
        gradient_outputs : np.ndarray
            The gradient of the errors with respect to the layer's outputs.
        learning_rate : float
            The learning rate to be used for adjusting the parameters.

        Raises
        ------
        NotImplementedError
            If the method is not implemented by subclasses.

        Returns
        -------
        gradient_inputs : np.ndarray
            The gradient of the errors with respect to the layer's inputs.
        """

        raise NotImplementedError


class FullyConnected(Layer):
    """
    A fully connected (dense) layer in a neural network.

    Parameters
    ----------
    num_inputs : int
        The number of input neurons.
    num_outputs : int
        The number of output neurons.

    Attributes
    ----------
    inputs : np.ndarray
        The input data to the layer, initialized as None.
    outputs : np.ndarray
        The output data from the layer, initialized as None.
    num_inputs : int
        The number of input neurons.
    num_outputs : int
        The number of output neuros.
    """

    def __init__(self, num_inputs: int, num_outputs: int) -> None:
        """
        Initializes a new instance of the FullyConnected class.

        Parameters
        ----------
        num_inputs : int
            The number of input neurons.
        num_outputs : int
            The number of output neurons.
        """

        super().__init__()

        self.num_inputs = num_inputs
        self.num_outputs = num_outputs

        self.init_randomly()

    def init_randomly(self) -> None:
        self.bias = np.random.rand(1, self.num_outputs) - 0.5
        self.weights = np.random.rand(self.num_inputs, self.num_outputs) - 0.5

    def dimension_compatibility(self, shape_inputs: Tuple[int]) -> Tuple[int]:
        if shape_inputs[1] != np.shape(self.weights)[0]:
            return None

        if shape_inputs[0] != np.shape(self.bias)[0]:
            return None

        return np.shape(self.bias)

    def forward_propagation(self, inputs: np.ndarray) -> np.ndarray:
        self.inputs = inputs
        self.outputs = self.inputs @ self.weights + self.bias

        return self.outputs

    def backward_propagation(self, gradient_outputs: np.ndarray,
                                   learning_rate: float) -> np.ndarray:

        gradient_inputs = gradient_outputs @ self.weights.T
        gradient_weights = self.inputs.T @ gradient_outputs

        # Update parameters by gradient descent
        self.bias -= learning_rate * gradient_outputs
        self.weights -= learning_rate * gradient_weights

        return gradient_inputs

    def __repr__(self) -> str:
        """
        Returns a string representation of the FullyConnected layer.

        Returns
        -------
        string : str
            String representation of the FullyConnected layer.
        """

        return f"FullyConnected({self.num_inputs}, {self.num_outputs})"
