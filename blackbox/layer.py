import numpy as np

from typing import Tuple, Union

from blackbox.optimizer import Optimizer


class Layer:
    """
    Base class for all layers in a neural network.

    Attributes
    ----------
    inputs : np.ndarray
        Stores the input tensor received by the layer in forward propagation.
    outputs : np.ndarray
        Stores the output tensor generated by the layer in forward propagation.
    """

    def __init__(self) -> None:
        """
        Initializes a new instance of the Layer class.
        """

        self.inputs = None
        self.outputs = None

    def dimension_compatibility(self, shape_inputs: Tuple[int, ...]) -> Union[Tuple[int, ...], None]:
        """
        Checks the compatibility of the input tensor with the layer's requirements.

        Parameters
        ----------
        shape_inputs : Tuple[int, ...]
            Input tensor dimensions.

        Returns
        -------
        shape_outputs : Tuple[int, ...] or None
            Output tensor dimensions or None if not compatible with the layer's requirements.

        Raises
        ------
        NotImplementedError
            If the method is not implemented by subclasses.
        """

        raise NotImplementedError

    def forward_propagation(self, inputs: np.ndarray) -> np.ndarray:
        """
        Performs forward propagation of the inputs through the layer.

        Parameters
        ----------
        inputs : np.ndarray
            Layer input tensor.

        Returns
        -------
        outputs : np.ndarray
            Layer output tensor.

        Raises
        ------
        NotImplementedError
            If the method is not implemented by subclasses.
        """

        raise NotImplementedError

    def backward_propagation(self, gradient_outputs: np.ndarray,
                                   optimizer: Optimizer) -> np.ndarray:
        """
        Performs backward propagation to adjust the parameters of the layer.

        Parameters
        ----------
        gradient_outputs : np.ndarray
            The gradient of the errors with respect to the layer's outputs.
        optimizer : Optimizer
            The iterative optimizer for updating the learnable parameters.

        Returns
        -------
        gradient_inputs : np.ndarray
            The gradient of the errors with respect to the layer's inputs.

        Raises
        ------
        NotImplementedError
            If the method is not implemented by subclasses.
        """

        raise NotImplementedError

    def __repr__(self) -> str:
        """
        Returns a string representation of the layer.

        Returns
        -------
        string : str
            String representation of the layer.
        """

        return f"{type(self).__name__}()"


class FullyConnected(Layer):
    """
    A fully connected (dense) layer in a neural network.

    Parameters
    ----------
    num_inputs : int
        The number of input neurons.
    num_outputs : int
        The number of output neurons.

    Attributes
    ----------
    num_inputs : int
        The number of input neurons.
    num_outputs : int
        The number of output neurons.
    inputs : np.ndarray
        Stores the input tensor received by the layer in forward propagation.
    outputs : np.ndarray
        Stores the output tensor generated by the layer in forward propagation.
    biases : np.ndarray
        The biases of the layer, initialized randomly in the range [-0.5, 0.5].
    weights : np.ndarray
        The weights of the layer, initialized randomly in the range [-0.5, 0.5].
    """

    def __init__(self, num_inputs: int, num_outputs: int) -> None:
        """
        Initializes a new instance of the FullyConnected layer.

        Parameters
        ----------
        num_inputs : int
            The number of input neurons.
        num_outputs : int
            The number of output neurons.
        """

        super().__init__()

        self.num_inputs = num_inputs
        self.num_outputs = num_outputs

        # Randomly initializes the parameters of the layer
        self.biases = np.random.rand(1, self.num_outputs) - 0.5
        self.weights = np.random.rand(self.num_inputs, self.num_outputs) - 0.5

    def dimension_compatibility(self, shape_inputs: Tuple[int, ...]) -> Union[Tuple[int, ...], None]:
        if shape_inputs[1] != np.shape(self.weights)[0]:
            return None

        if shape_inputs[0] != np.shape(self.biases)[0]:
            return None

        return np.shape(self.biases)

    def forward_propagation(self, inputs: np.ndarray) -> np.ndarray:
        self.inputs = inputs
        self.outputs = self.inputs @ self.weights + self.biases

        return self.outputs

    def backward_propagation(self, gradient_outputs: np.ndarray,
                                   optimizer: Optimizer) -> np.ndarray:

        gradient_inputs = gradient_outputs @ self.weights.T
        gradient_weights = self.inputs.T @ gradient_outputs

        # Updates the parameters in place
        optimizer.update(self.biases, gradient_outputs)
        optimizer.update(self.weights, gradient_weights)

        return gradient_inputs

    def __repr__(self) -> str:
        return f"FullyConnected({self.num_inputs}, {self.num_outputs})"
